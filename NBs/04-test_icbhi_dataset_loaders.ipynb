{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d49a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b666169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ls.config.loader import load_config\n",
    "import IPython.display as ipd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2f6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset config: {'name': 'icbhi', 'data_folder': '/home/AIoT04/Datasets/icbhi_dataset', 'cycle_metadata_path': '/home/AIoT04/Datasets/icbhi_dataset/icbhi_metadata.csv', 'class_split': 'lungsound', 'split_strategy': 'official', 'test_fold': 0, 'multi_label': True, 'n_cls': 4, 'weighted_sampler': True, 'batch_size': 16, 'num_workers': 2, 'h': 128, 'w': 1024}\n",
      "Audio config: {'sample_rate': 16000, 'desired_length': 10.0, 'remove_dc': True, 'normalize': False, 'pad_type': 'repeat', 'use_fade': True, 'fade_samples_ratio': 64, 'n_mels': 128, 'frame_length': 40, 'frame_shift': 10, 'low_freq': 100, 'high_freq': 8000, 'window_type': 'hanning', 'use_energy': False, 'dither': 0.0, 'mel_norm': 'mit', 'resz': 1.0, 'raw_augment': 1, 'wave_aug': [{'type': 'Crop', 'sampling_rate': 16000, 'zone': [0.0, 1.0], 'coverage': 1.0, 'p': 0.0}, {'type': 'Noise', 'color': 'white', 'p': 0.1}, {'type': 'Speed', 'factor': [0.9, 1.1], 'p': 0.1}, {'type': 'Loudness', 'factor': [0.5, 2.0], 'p': 0.1}, {'type': 'VTLP', 'sampling_rate': 16000, 'zone': [0.0, 1.0], 'fhi': 4800, 'factor': [0.9, 1.1], 'p': 0.1}, {'type': 'Pitch', 'sampling_rate': 16000, 'factor': [-1, 3], 'p': 0.0}], 'spec_aug': [{'type': 'SpecAugment', 'policy': 'icbhi_ast_sup', 'mask': 'zero', 'p': 0.3}]}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. load config ---\n",
    "cfg = load_config(\"../configs/config.yaml\")\n",
    "\n",
    "print(\"Dataset config:\", cfg.dataset)\n",
    "print(\"Audio config:\", cfg.audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ddf89",
   "metadata": {},
   "source": [
    "## Test DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58543b",
   "metadata": {},
   "source": [
    "### ICBHI Dataloader using only test set as the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdbb78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transforms] Input spectrogram resize factor: 1.0, target size: (128, 1024)\n",
      "[Transforms] Input spectrogram resize factor: 1.0, target size: (128, 1024)\n",
      "[ICBHI] Loaded cycle metadata TSV: 6898 rows\n",
      "[ICBHI] #Sites=7, #Devices=4\n",
      "[ICBHI] Sites Found: {'Al': 0, 'Ar': 1, 'Ll': 2, 'Lr': 3, 'Pl': 4, 'Pr': 5, 'Tc': 6}\n",
      "[ICBHI] Devices Found: {'AKGC417L': 0, 'Litt3200': 1, 'LittC2SE': 2, 'Meditron': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/data/iotlab/AIoT/konto/envs/icbhi-ast/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/storage/data/iotlab/AIoT/konto/envs/icbhi-ast/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ICBHI] Extracted 4142 cycles from 539 recordings\n",
      "[ICBHI] Metadata join missing: 0 (strict join; should be 0)\n",
      "[ICBHI] Input spectrogram shape: (997, 128, 1)\n",
      "[ICBHI] 4142 cycles\n",
      "  Class 0: 2063 (49.8%)\n",
      "  Class 1: 1215 (29.3%)\n",
      "  Class 2: 501 (12.1%)\n",
      "  Class 3: 363 (8.8%)\n",
      "[ICBHI] Loaded cycle metadata TSV: 6898 rows\n",
      "[ICBHI] #Sites=7, #Devices=4\n",
      "[ICBHI] Sites Found: {'Al': 0, 'Ar': 1, 'Ll': 2, 'Lr': 3, 'Pl': 4, 'Pr': 5, 'Tc': 6}\n",
      "[ICBHI] Devices Found: {'AKGC417L': 0, 'Litt3200': 1, 'LittC2SE': 2, 'Meditron': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/data/iotlab/AIoT/konto/envs/icbhi-ast/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/storage/data/iotlab/AIoT/konto/envs/icbhi-ast/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ICBHI] Extracted 2756 cycles from 381 recordings\n",
      "[ICBHI] Metadata join missing: 0 (strict join; should be 0)\n",
      "[ICBHI] Input spectrogram shape: (997, 128, 1)\n",
      "[ICBHI] 2756 cycles\n",
      "  Class 0: 1579 (57.3%)\n",
      "  Class 1: 649 (23.5%)\n",
      "  Class 2: 385 (14.0%)\n",
      "  Class 3: 143 (5.2%)\n"
     ]
    }
   ],
   "source": [
    "# Regular training\n",
    "from ls.data.dataloaders import build_dataloaders\n",
    "\n",
    "train_loader, test_loader = build_dataloaders(cfg.dataset, cfg.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfe5ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 128, 1024]) torch.Size([16, 2]) ['121_1p1_Tc_sc_Meditron', '159_1b1_Al_sc_Meditron', '172_2b5_Al_mc_AKGC417L', '130_1p2_Ar_mc_AKGC417L', '158_1p3_Lr_mc_AKGC417L', '154_2b4_Al_mc_AKGC417L', '193_7b3_Ll_mc_AKGC417L', '159_1b1_Ll_sc_Meditron', '192_2b3_Al_mc_LittC2SE', '200_2p4_Ar_mc_AKGC417L', '162_1b2_Ar_mc_AKGC417L', '222_1b1_Lr_sc_Meditron', '180_1b4_Pr_mc_AKGC417L', '130_3p3_Tc_mc_AKGC417L', '138_1p4_Lr_mc_AKGC417L', '158_1p3_Al_mc_AKGC417L']\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[\"input_values\"].shape, batch[\"label\"].shape, batch[\"filename\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab19517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values', 'audio', 'filename', 'cycle_index', 'pid', 'duration', 'start_time', 'end_time', 'site', 'device', 'site_id', 'device_id', 'age', 'bmi', 'm_rest', 'label'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c092f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 1024]),\n",
       " tensor([0., 0.]),\n",
       " '121_1p1_Tc_sc_Meditron',\n",
       " tensor(2.4420, dtype=torch.float64),\n",
       " tensor(15.0790, dtype=torch.float64),\n",
       " tensor(17.5210, dtype=torch.float64),\n",
       " 'Meditron',\n",
       " tensor(3),\n",
       " tensor(6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_values'][0].shape, batch['label'][0], batch['filename'][0], batch['duration'][0], batch['start_time'][0], batch['end_time'][0], batch['device'][0], batch['device_id'][0], batch['site_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def inspect_batch_balance(train_loader, n_batches=100):\n",
    "    pattern_counter = Counter()\n",
    "    total = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        labels = batch[\"label\"].cpu().numpy()\n",
    "        # For multi-label mode (2 columns)\n",
    "        if labels.ndim == 2:\n",
    "            patterns = [tuple(v) for v in labels]\n",
    "        else:\n",
    "            patterns = [int(v) for v in labels]\n",
    "        pattern_counter.update(patterns)\n",
    "        total += len(patterns)\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nChecked {total} samples from {n_batches} batches\")\n",
    "    for pat, c in sorted(pattern_counter.items()):\n",
    "        print(f\"Pattern {pat}: {c} ({100*c/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab99993",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_batch_balance(train_loader, n_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a391b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_equal(a, b, atol=1e-6, rtol=1e-5):\n",
    "    return torch.allclose(a, b, atol=atol, rtol=rtol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d036e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect one sample ---\n",
    "idx = torch.randint(cfg.dataset.batch_size, (1,)).item()\n",
    "print(f\"Inspecting sample index {idx} in the batch\")\n",
    "print(\"Keys:\", batch.keys())\n",
    "print(\"Filename:\", batch[\"filename\"][idx])\n",
    "print(\"Cycle index:\", batch[\"cycle_index\"][idx])\n",
    "print(\"Label:\", batch[\"label\"][idx])\n",
    "print(\"Duration:\", batch[\"duration\"][idx])\n",
    "print(\"Start-End:\", batch[\"start_time\"][idx], \"-\", batch[\"end_time\"][idx])\n",
    "print(\"Crackle/Wheeze:\", batch[\"label\"][idx][0], batch[\"label\"][idx][1])\n",
    "# print(sample[\"aug_audio\"].shape, sample[\"aug_fbank\"].shape, sample[\"audio\"].shape, sample[\"fbank\"].shape)\n",
    "print(\"Waveform shape:\", batch[\"audio\"][idx].shape)\n",
    "print(\"Mel image shape:\", batch[\"input_values\"][idx].shape)\n",
    "# print(f\"Waveform augmented: {not tensors_equal(batch['audio'][idx].view(-1), batch['aug_audio'][idx].view(-1))}\")\n",
    "# print(f\"Mel augmented: {not tensors_equal(batch['fbank'][idx].view(-1), batch['aug_fbank'][idx].view(-1))}\")\n",
    "\n",
    "# --- 4. Plot waveform ---\n",
    "waveform = batch[\"audio\"][idx].squeeze().numpy()\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(waveform)\n",
    "plt.title(f\"Waveform ({batch['filename'][idx]} - cycle {batch['cycle_index'][idx]})\")\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Plot mel spectrogram ---\n",
    "mel = batch[\"input_values\"][idx].squeeze(0)  # [freq, time] for imshow\n",
    "freq_axis = np.linspace(0, cfg.audio.sample_rate // 2, mel.shape[0])\n",
    "time_axis = np.arange(mel.shape[1]) * cfg.audio.frame_shift / 1000  # in seconds\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(mel, origin=\"lower\", aspect=\"auto\", extent=[0, time_axis[-1], 0, freq_axis[-1]], cmap=\"viridis\")\n",
    "plt.title(\"Mel filterbank\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "ipd.Audio(waveform, rate=cfg.audio.sample_rate)  # listen to the sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ls.data.dataloaders import compute_and_cache_stats\n",
    "\n",
    "mean, std = compute_and_cache_stats(\n",
    "    train_loader.dataset, cache_file=\"train_stats.json\", batch_size=cfg.dataset.batch_size,\n",
    "    num_workers=cfg.dataset.num_workers\n",
    ")\n",
    "print(f\"Dataset mean: {mean:.4f}, std: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1f34",
   "metadata": {},
   "source": [
    "## Stratified Grouped KFold Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ls.data.dataloaders import build_train_val_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149950ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or: k-fold CV on train set\n",
    "folds, test_loader = build_train_val_kfold(\n",
    "    cfg.dataset, cfg.audio, n_splits=5, max_retries=50, seed=cfg.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_loader, val_loader) in enumerate(folds):\n",
    "    # print(f\"Training fold {i+1}\")\n",
    "    for batch in train_loader:\n",
    "        # Process each batch\n",
    "        print(batch[\"input_values\"].shape, batch[\"label\"].shape, batch[\"filename\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e4bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icbhi-ast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
