{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38be08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247b8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ls.config.loader import load_config\n",
    "import IPython.display as ipd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744e4978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset config: {'name': 'icbhi', 'data_folder': '/Users/gkont/Downloads/Datasets/icbhi_dataset', 'cycle_metadata_path': '/Users/gkont/Downloads/Datasets/icbhi_dataset/icbhi_metadata.csv', 'class_split': 'lungsound', 'split_strategy': 'official', 'test_fold': 0, 'multi_label': True, 'n_cls': 4, 'weighted_sampler': True, 'batch_size': 8, 'num_workers': 0, 'h': 128, 'w': 1024}\n",
      "Audio config: {'sample_rate': 16000, 'desired_length': 10.0, 'remove_dc': True, 'normalize': False, 'pad_type': 'repeat', 'use_fade': True, 'fade_samples_ratio': 64, 'n_mels': 128, 'frame_length': 40, 'frame_shift': 10, 'low_freq': 100, 'high_freq': 5000, 'window_type': 'hanning', 'use_energy': False, 'dither': 0.0, 'mel_norm': 'mit', 'resz': 1.0, 'raw_augment': 1, 'wave_aug': [{'type': 'Crop', 'sampling_rate': 16000, 'zone': [0.0, 1.0], 'coverage': 1.0, 'p': 0.0}, {'type': 'Noise', 'color': 'white', 'p': 0.1}, {'type': 'Speed', 'factor': [0.9, 1.1], 'p': 0.1}, {'type': 'Loudness', 'factor': [0.5, 2.0], 'p': 0.1}, {'type': 'VTLP', 'sampling_rate': 16000, 'zone': [0.0, 1.0], 'fhi': 4800, 'factor': [0.9, 1.1], 'p': 0.1}, {'type': 'Pitch', 'sampling_rate': 16000, 'factor': [-1, 3], 'p': 0.0}], 'spec_aug': [{'type': 'SpecAugment', 'policy': 'icbhi_ast_sup', 'mask': 'zero', 'p': 0.3}]}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. load config ---\n",
    "cfg = load_config(\"../configs/config.yaml\")\n",
    "\n",
    "print(\"Dataset config:\", cfg.dataset)\n",
    "print(\"Audio config:\", cfg.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4920a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transforms] Input spectrogram resize factor: 1.0, target size: (128, 1024)\n",
      "[Transforms] Input spectrogram resize factor: 1.0, target size: (128, 1024)\n",
      "[ICBHI] Loaded cycle metadata TSV: 6898 rows\n",
      "[ICBHI] #Sites=7, #Devices=4\n",
      "[ICBHI] Sites Found: {'Al': 0, 'Ar': 1, 'Ll': 2, 'Lr': 3, 'Pl': 4, 'Pr': 5, 'Tc': 6}\n",
      "[ICBHI] Devices Found: {'AKGC417L': 0, 'Litt3200': 1, 'LittC2SE': 2, 'Meditron': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/icbhi-ast/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ICBHI] Extracted 4142 cycles from 539 recordings\n",
      "[ICBHI] Metadata join missing: 0 (strict join; should be 0)\n",
      "[ICBHI] Input spectrogram shape: (997, 128, 1)\n",
      "[ICBHI] 4142 cycles\n",
      "  Class 0: 2063 (49.8%)\n",
      "  Class 1: 1215 (29.3%)\n",
      "  Class 2: 501 (12.1%)\n",
      "  Class 3: 363 (8.8%)\n",
      "[ICBHI] Loaded cycle metadata TSV: 6898 rows\n",
      "[ICBHI] #Sites=7, #Devices=4\n",
      "[ICBHI] Sites Found: {'Al': 0, 'Ar': 1, 'Ll': 2, 'Lr': 3, 'Pl': 4, 'Pr': 5, 'Tc': 6}\n",
      "[ICBHI] Devices Found: {'AKGC417L': 0, 'Litt3200': 1, 'LittC2SE': 2, 'Meditron': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/icbhi-ast/lib/python3.11/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ICBHI] Extracted 2756 cycles from 381 recordings\n",
      "[ICBHI] Metadata join missing: 0 (strict join; should be 0)\n",
      "[ICBHI] Input spectrogram shape: (997, 128, 1)\n",
      "[ICBHI] 2756 cycles\n",
      "  Class 0: 1579 (57.3%)\n",
      "  Class 1: 649 (23.5%)\n",
      "  Class 2: 385 (14.0%)\n",
      "  Class 3: 143 (5.2%)\n"
     ]
    }
   ],
   "source": [
    "# Regular training\n",
    "from ls.data.dataloaders import build_dataloaders\n",
    "\n",
    "train_loader, test_loader = build_dataloaders(cfg.dataset, cfg.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cbfe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ea8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/icbhi-ast/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8, 1, 128, 1024]) torch.float32\n",
      "device_id: torch.Size([8]) torch.int64\n",
      "site_id: torch.Size([8]) torch.int64\n",
      "m_rest: torch.Size([8, 3]) torch.float32\n",
      "y: torch.Size([8, 2]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "x = batch[\"input_values\"].to(DEVICE)      # (B, 1, F, T)\n",
    "device_id = batch[\"device_id\"].to(DEVICE) # (B,)\n",
    "site_id   = batch[\"site_id\"].to(DEVICE)   # (B,)\n",
    "m_rest    = batch[\"m_rest\"].to(DEVICE)    # (B, rest_dim)\n",
    "y         = batch[\"label\"].to(DEVICE)     # (B,2) for multilabel\n",
    "\n",
    "print(\"x:\", x.shape, x.dtype)\n",
    "print(\"device_id:\", device_id.shape, device_id.dtype)\n",
    "print(\"site_id:\", site_id.shape, site_id.dtype)\n",
    "print(\"m_rest:\", m_rest.shape, m_rest.dtype)\n",
    "print(\"y:\", y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db561332",
   "metadata": {},
   "source": [
    "## Baseline, Naive Concatenation & Projection Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22415964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from ls.models.ast import ASTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62a656bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline: AST encoder -> MLP -> 2 logits (crackle, wheeze).\n",
    "    Matches Section 2.1 (Baseline Model) in your Methods.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ast_kwargs: dict,\n",
    "        hidden_dim: int = 64,\n",
    "        dropout_p: float = 0.3,\n",
    "        num_labels: int = 2,      # crackle, wheeze\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # AST as backbone-only encoder (returns h_CLS)\n",
    "        self.ast = ASTModel(\n",
    "            backbone_only=True,\n",
    "            **ast_kwargs\n",
    "        )\n",
    "        \n",
    "        D = self.ast.original_embedding_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(D),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(D, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim, num_labels)   # logits (no sigmoid)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) or (B, 1, F, T)\n",
    "        returns: (B, 2) logits (crackle, wheeze)\n",
    "        \"\"\"\n",
    "        h_cls = self.ast.forward_features(x)           # (B, D), your h_CLS\n",
    "        print(f\"h_cls shape: {h_cls.shape}\")  # (B, D)\n",
    "        logits = self.classifier(h_cls)\n",
    "        print(f\"logits shape: {logits.shape}\")  # (B, 2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "870d0722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: True\n",
      "Loading AudioSet pretrained model from /Users/gkont/Documents/Code/pretrained_models/audioset_10_10_0.4593.pth\n",
      "No mismatch for key: v.cls_token\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.pos_embed\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.dist_token\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.patch_embed.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.patch_embed.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.0.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.1.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.2.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.3.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.4.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.5.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.6.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.7.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.8.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.9.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.10.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.norm1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.norm1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.attn.qkv.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.attn.qkv.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.attn.proj.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.attn.proj.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.norm2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.norm2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.mlp.fc1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.mlp.fc1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.mlp.fc2.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.blocks.11.mlp.fc2.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.norm.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.norm.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.head.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.head.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.head_dist.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: v.head_dist.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: mlp_head.0.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: mlp_head.0.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: mlp_head.1.weight\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "No mismatch for key: mlp_head.1.bias\n",
      "No mlp_head weights loaded from AudioSet checkpoint.\n",
      "Loaded 157/161 tensors from AudioSet checkpoint\n",
      "The remaining 4 tensors are randomly initialized and will be trained from scratch.\n",
      "frequency stride=10, time stride=10\n",
      "number of patches=1212\n"
     ]
    }
   ],
   "source": [
    "ast_kwargs = dict(\n",
    "    label_dim=2,          # unused since backbone_only=True\n",
    "    fstride=10,\n",
    "    tstride=10,\n",
    "    input_fdim=128,\n",
    "    input_tdim=1024,\n",
    "    imagenet_pretrain=True,\n",
    "    audioset_pretrain=True,\n",
    "    audioset_ckpt_path='/Users/gkont/Documents/Code/pretrained_models/audioset_10_10_0.4593.pth',\n",
    "    model_size='base384',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "baseline_model = ASTBaseline(ast_kwargs, hidden_dim=64, dropout_p=0.3, num_labels=2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f131c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTBaseline(\n",
      "  (ast): ASTModel(\n",
      "    (reg_dropout): Dropout(p=0.3, inplace=False)\n",
      "    (v): DistilledVisionTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pre_logits): Identity()\n",
      "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "      (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
      "    )\n",
      "    (mlp_head): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): Linear(in_features=768, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Dropout(p=0.3, inplace=False)\n",
      "      (5): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): Linear(in_features=768, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72c16b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 128, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3f67403",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "out = baseline_model(x)  # (B, 2)\n",
    "print(out.shape)  # torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c1af4",
   "metadata": {},
   "source": [
    "### Projected added metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2931d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTWithMetadataProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Metadata projection fusion:\n",
    "        m = [E_dev(device_id), E_site(site_id), m_rest]\n",
    "        m' = W m\n",
    "        h_tilde = h_CLS + m'\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ast_kwargs: dict,\n",
    "        num_devices: int,\n",
    "        num_sites: int,\n",
    "        dev_emb_dim: int = 4,\n",
    "        site_emb_dim: int = 4,\n",
    "        rest_dim: int = 5,\n",
    "        hidden_dim: int = 64,\n",
    "        dropout_p: float = 0.3,\n",
    "        num_labels: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ast = ASTModel(backbone_only=True, **ast_kwargs)\n",
    "        D = self.ast.original_embedding_dim\n",
    "\n",
    "        # categorical encoders\n",
    "        self.dev_emb  = nn.Embedding(num_devices, dev_emb_dim)\n",
    "        self.site_emb = nn.Embedding(num_sites, site_emb_dim)\n",
    "\n",
    "        meta_dim = dev_emb_dim + site_emb_dim + rest_dim\n",
    "        self.metadata_proj = nn.Linear(meta_dim, D)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(D),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(D, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, device_id, site_id, m_rest):\n",
    "        \"\"\"\n",
    "        x:          (B, 1, F, T)\n",
    "        device_id:  (B,) long\n",
    "        site_id:    (B,) long\n",
    "        m_rest:     (B, rest_dim) float\n",
    "        \"\"\"\n",
    "        h_cls = self.ast(x)                    # (B, D)\n",
    "\n",
    "        dev  = self.dev_emb(device_id)         # (B, d_dev)\n",
    "        site = self.site_emb(site_id)          # (B, d_site)\n",
    "\n",
    "        m = torch.cat([dev, site, m_rest], dim=-1)  # (B, meta_dim)\n",
    "        m_prime = self.metadata_proj(m)              # (B, D)\n",
    "\n",
    "        h_tilde = h_cls + m_prime\n",
    "        logits = self.classifier(h_tilde)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f59c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][\"m_rest\"].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffe26a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTWithMetadataProjection(\n",
      "  (ast): ASTModel(\n",
      "    (reg_dropout): Dropout(p=0.3, inplace=False)\n",
      "    (v): DistilledVisionTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pre_logits): Identity()\n",
      "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "      (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
      "    )\n",
      "    (mlp_head): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): Linear(in_features=768, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Dropout(p=0.3, inplace=False)\n",
      "      (5): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dev_emb): Embedding(4, 4)\n",
      "  (site_emb): Embedding(7, 4)\n",
      "  (metadata_proj): Linear(in_features=11, out_features=768, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): Linear(in_features=768, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ast_kwargs = dict(\n",
    "    label_dim=2,          # unused since backbone_only=True\n",
    "    fstride=10,\n",
    "    tstride=10,\n",
    "    input_fdim=128,\n",
    "    input_tdim=1024,\n",
    "    imagenet_pretrain=True,\n",
    "    audioset_pretrain=True,\n",
    "    audioset_ckpt_path='/Users/gkont/Documents/Code/pretrained_models/audioset_10_10_0.4593.pth',\n",
    "    model_size='base384',\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# If your dataset returns: m_rest = [sex, age, bmi, duration, bmi_missing]\n",
    "num_devices = 4\n",
    "num_sites = 7\n",
    "rest_dim = train_loader.dataset[0][\"m_rest\"].numel()\n",
    "\n",
    "meta_model = ASTWithMetadataProjection(\n",
    "    ast_kwargs=ast_kwargs,\n",
    "    num_devices=num_devices,\n",
    "    num_sites=num_sites,\n",
    "    dev_emb_dim=4,\n",
    "    site_emb_dim=4,\n",
    "    rest_dim=rest_dim,\n",
    "    hidden_dim=64,\n",
    "    dropout_p=0.3,\n",
    "    num_labels=2\n",
    ").to(DEVICE)\n",
    "\n",
    "print(meta_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee47d656",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m logits = \u001b[43mmeta_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msite_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_rest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits:\u001b[39m\u001b[33m\"\u001b[39m, logits.shape, logits.dtype)  \u001b[38;5;66;03m# (B,2)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/icbhi-ast/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/icbhi-ast/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mASTWithMetadataProjection.forward\u001b[39m\u001b[34m(self, x, device_id, site_id, m_rest)\u001b[39m\n\u001b[32m     53\u001b[39m m = torch.cat([dev, site, m_rest], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, meta_dim)\u001b[39;00m\n\u001b[32m     54\u001b[39m m_prime = \u001b[38;5;28mself\u001b[39m.metadata_proj(m)              \u001b[38;5;66;03m# (B, D)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m h_tilde = \u001b[43mh_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_prime\u001b[49m\n\u001b[32m     57\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(h_tilde)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "logits = meta_model(x, device_id, site_id, m_rest)\n",
    "print(\"logits:\", logits.shape, logits.dtype)  # (B,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTWithMetadataProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Metadata projection fusion:\n",
    "        m -> m' in R^D\n",
    "        h_tilde = h_CLS + m'\n",
    "        h_tilde -> MLP -> logits\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ast_kwargs: dict,\n",
    "        metadata_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        dropout_p: float = 0.3,\n",
    "        num_labels: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ast = ASTModel(\n",
    "            backbone_only=True,\n",
    "            **ast_kwargs\n",
    "        )\n",
    "        D = self.ast.original_embedding_dim\n",
    "        self.metadata_dim = metadata_dim\n",
    "\n",
    "        # Linear projection m -> m' in R^D\n",
    "        self.metadata_proj = nn.Linear(metadata_dim, D)\n",
    "\n",
    "        # Same style classifier as baseline\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(D),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(D, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) or (B, 1, F, T)\n",
    "        m: (B, M)\n",
    "        \"\"\"\n",
    "        h_cls = self.ast(x)               # (B, D)\n",
    "        m_prime = self.metadata_proj(m)   # (B, D)\n",
    "        h_tilde = h_cls + m_prime         # (B, D)  -- Eq. (11)\n",
    "        logits = self.classifier(h_tilde)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "astproj = ASTWithMetadataProjection(ast_kwargs, metadata_dim=10, hidden_dim=64, dropout_p=0.3, num_labels=2).to(DEVICE)\n",
    "print(astproj)\n",
    "out = astproj(dummy_input, metadata)  # (2, 2)\n",
    "print(out.shape)  # torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab1d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0827f6d",
   "metadata": {},
   "source": [
    "### FiLM: Metadata conditioning inside the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTFiLM(ASTModel):\n",
    "    \"\"\"\n",
    "    AST with FiLM conditioning on selected Transformer layers.\n",
    "\n",
    "    - Uses ASTModel's __init__ to build the DeiT backbone.\n",
    "    - Adds a metadata encoder g(m).\n",
    "    - For a set of layers B, generates layer-specific gamma_l, beta_l.\n",
    "    - Applies FiLM after the MHSA residual, before the FFN (as per your equations).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_dim: int,\n",
    "        conditioned_layers=(0, 1, 2, 3),   # indices into self.v.blocks\n",
    "        metadata_hidden_dim: int = 64,\n",
    "        film_hidden_dim: int = 64,\n",
    "        num_labels: int = 2,\n",
    "        dropout_p: float = 0.3,\n",
    "        ast_kwargs: dict = None,\n",
    "    ):\n",
    "        ast_kwargs = ast_kwargs or {}\n",
    "        super().__init__(backbone_only=True, **ast_kwargs)\n",
    "\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.conditioned_layers = sorted(list(conditioned_layers))\n",
    "        self.conditioned_layers_set = set(self.conditioned_layers)\n",
    "\n",
    "        D = self.original_embedding_dim\n",
    "        self.num_layers = len(self.v.blocks)\n",
    "\n",
    "        # Metadata encoder h_m = g(m)\n",
    "        self.metadata_encoder = nn.Sequential(\n",
    "            nn.LayerNorm(metadata_dim),\n",
    "            nn.Linear(metadata_dim, metadata_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(metadata_hidden_dim, film_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # One FiLM generator per conditioned layer:\n",
    "        # f_l: h_m -> [gamma_l || beta_l] in R^{2D}\n",
    "        self.film_generators = nn.ModuleDict()\n",
    "        for l in self.conditioned_layers:\n",
    "            self.film_generators[str(l)] = nn.Linear(film_hidden_dim, 2 * D)\n",
    "\n",
    "        # Classification head (same as baseline)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(D),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(D, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(64, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x, m):\n",
    "        \"\"\"\n",
    "        x: (B, 1, F, T)  (we'll handle 3D in forward())\n",
    "        m: (B, M) metadata\n",
    "        returns: h_CLS in R^D (FiLM-conditioned)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Patch embedding: (B, N, D)\n",
    "        x = self.v.patch_embed(x)\n",
    "\n",
    "        # CLS + dist tokens and positional embeddings\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "\n",
    "        # Encode metadata once, then generate FiLM params per layer\n",
    "        h_m = self.metadata_encoder(m)   # (B, film_hidden_dim)\n",
    "\n",
    "        gamma = {}\n",
    "        beta = {}\n",
    "        for l in self.conditioned_layers:\n",
    "            film = self.film_generators[str(l)](h_m)  # (B, 2D)\n",
    "            g_l, b_l = film.chunk(2, dim=-1)          # (B, D), (B, D)\n",
    "            gamma[l] = g_l\n",
    "            beta[l] = b_l\n",
    "\n",
    "        # Manually unroll each Transformer block\n",
    "        for layer_idx, blk in enumerate(self.v.blocks):\n",
    "            # MHSA sublayer with pre-norm\n",
    "            attn_out = blk.attn(blk.norm1(x))\n",
    "            x = x + blk.drop_path(attn_out)   # Z_tilde_l\n",
    "\n",
    "            # Apply FiLM after MHSA for selected layers\n",
    "            if layer_idx in self.conditioned_layers_set:\n",
    "                print(f\"Applying FiLM at layer {layer_idx}\")\n",
    "                g_l = gamma[layer_idx].unsqueeze(1)   # (B, 1, D) -> broadcast over tokens\n",
    "                b_l = beta[layer_idx].unsqueeze(1)    # (B, 1, D)\n",
    "                x = g_l * x + b_l                     # Eq. (1415): hat{Z}_l\n",
    "\n",
    "            # FFN + residual\n",
    "            x = x + blk.drop_path(blk.mlp(blk.norm2(x)))\n",
    "\n",
    "        x = self.v.norm(x)\n",
    "        h_cls = (x[:, 0] + x[:, 1]) / 2  # pooled token\n",
    "        return h_cls\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        \"\"\"\n",
    "        x: (B, T, F) or (B, 1, F, T)\n",
    "        m: (B, M) metadata\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1).transpose(2, 3)  # (B, 1, F, T)\n",
    "\n",
    "        h_cls = self.forward_features(x, m)\n",
    "        logits = self.classifier(h_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "astfilm = ASTFiLM(\n",
    "    ast_kwargs=ast_kwargs, \n",
    "    metadata_dim=10,\n",
    "    conditioned_layers=range(12), # all layers\n",
    "    metadata_hidden_dim=64, \n",
    "    film_hidden_dim=64,\n",
    "    dropout_p=0.3, \n",
    "    num_labels=2\n",
    ").to(DEVICE)\n",
    "print(astfilm)\n",
    "metadata  = torch.randn(2, 10).to(DEVICE)  # (B, M)\n",
    "out = astfilm(dummy_input, metadata)  # (2, 2)\n",
    "print(out.shape)  # torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad639488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTFiLMPlusPlus(ASTModel):\n",
    "    \"\"\"\n",
    "    FiLM++: factor-aligned grouped FiLM with K=3 groups:\n",
    "      - device group    (D_dev)\n",
    "      - site group      (D_site)\n",
    "      - rest group      (D_rest)\n",
    "\n",
    "    Metadata inputs are passed separately as (m_dev, m_site, m_rest).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dev_metadata_dim: int,\n",
    "        site_metadata_dim: int,\n",
    "        rest_metadata_dim: int,\n",
    "        D_dev: int,\n",
    "        D_site: int,\n",
    "        conditioned_layers=(0, 1, 2, 3),\n",
    "        metadata_hidden_dim: int = 64,\n",
    "        film_hidden_dim: int = 64,\n",
    "        num_labels: int = 2,\n",
    "        dropout_p: float = 0.3,\n",
    "        ast_kwargs: dict = None,\n",
    "    ):\n",
    "        ast_kwargs = ast_kwargs or {}\n",
    "        super().__init__(backbone_only=True, **ast_kwargs)\n",
    "\n",
    "        self.conditioned_layers = sorted(list(conditioned_layers))\n",
    "        self.conditioned_layers_set = set(self.conditioned_layers)\n",
    "\n",
    "        D_total = self.original_embedding_dim\n",
    "        assert D_dev + D_site <= D_total, \"D_dev + D_site must be <= total embedding dim\"\n",
    "        D_rest = D_total - D_dev - D_site\n",
    "\n",
    "        self.D_dev = D_dev\n",
    "        self.D_site = D_site\n",
    "        self.D_rest = D_rest\n",
    "        self.D_total = D_total\n",
    "\n",
    "        # --- Metadata encoders for each factor ---\n",
    "        self.dev_encoder = nn.Sequential(\n",
    "            nn.LayerNorm(dev_metadata_dim),\n",
    "            nn.Linear(dev_metadata_dim, metadata_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(metadata_hidden_dim, film_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.site_encoder = nn.Sequential(\n",
    "            nn.LayerNorm(site_metadata_dim),\n",
    "            nn.Linear(site_metadata_dim, metadata_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(metadata_hidden_dim, film_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rest_encoder = nn.Sequential(\n",
    "            nn.LayerNorm(rest_metadata_dim),\n",
    "            nn.Linear(rest_metadata_dim, metadata_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(metadata_hidden_dim, film_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # --- FiLM generators per group and per conditioned layer ---\n",
    "        self.dev_film = nn.ModuleDict()\n",
    "        self.site_film = nn.ModuleDict()\n",
    "        self.rest_film = nn.ModuleDict()\n",
    "        for l in self.conditioned_layers:\n",
    "            self.dev_film[str(l)] = nn.Linear(film_hidden_dim, 2 * D_dev)\n",
    "            self.site_film[str(l)] = nn.Linear(film_hidden_dim, 2 * D_site)\n",
    "            self.rest_film[str(l)] = nn.Linear(film_hidden_dim, 2 * D_rest)\n",
    "\n",
    "        # Classification head (same style as baseline)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(D_total),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(D_total, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(64, num_labels),\n",
    "        )\n",
    "\n",
    "    def _apply_filmpp_grouped(self, x, gammas, betas):\n",
    "        \"\"\"\n",
    "        x:      (B, T, D_total)\n",
    "        gammas: dict with 'dev','site','rest' tensors (B, D_group)\n",
    "        betas:  same as above\n",
    "\n",
    "        Applies group-wise FiLM and returns (B, T, D_total).\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        D_dev, D_site, D_rest = self.D_dev, self.D_site, self.D_rest\n",
    "\n",
    "        x_dev, x_site, x_rest = torch.split(\n",
    "            x, [D_dev, D_site, D_rest], dim=-1\n",
    "        )  # each (B, T, D_group)\n",
    "\n",
    "        # Broadcast gammas/betas over tokens\n",
    "        g_dev = gammas[\"dev\"].unsqueeze(1)   # (B, 1, D_dev)\n",
    "        b_dev = betas[\"dev\"].unsqueeze(1)\n",
    "        g_site = gammas[\"site\"].unsqueeze(1) # (B, 1, D_site)\n",
    "        b_site = betas[\"site\"].unsqueeze(1)\n",
    "        g_rest = gammas[\"rest\"].unsqueeze(1) # (B, 1, D_rest)\n",
    "        b_rest = betas[\"rest\"].unsqueeze(1)\n",
    "\n",
    "        x_dev_hat = g_dev * x_dev + b_dev\n",
    "        x_site_hat = g_site * x_site + b_site\n",
    "        x_rest_hat = g_rest * x_rest + b_rest\n",
    "\n",
    "        x_hat = torch.cat([x_dev_hat, x_site_hat, x_rest_hat], dim=-1)  # (B, T, D_total)\n",
    "        return x_hat\n",
    "\n",
    "    def forward_features(self, x, m_dev, m_site, m_rest):\n",
    "        \"\"\"\n",
    "        x:      (B, 1, F, T)\n",
    "        m_dev:  (B, dev_metadata_dim)\n",
    "        m_site: (B, site_metadata_dim)\n",
    "        m_rest: (B, rest_metadata_dim)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Patch embedding\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "\n",
    "        # Encode metadata for each factor\n",
    "        h_dev = self.dev_encoder(m_dev)\n",
    "        h_site = self.site_encoder(m_site)\n",
    "        h_rest = self.rest_encoder(m_rest)\n",
    "\n",
    "        # Precompute gamma,beta for each conditioned layer\n",
    "        gamma_dev, beta_dev = {}, {}\n",
    "        gamma_site, beta_site = {}, {}\n",
    "        gamma_rest, beta_rest = {}, {}\n",
    "\n",
    "        for l in self.conditioned_layers:\n",
    "            dev_params = self.dev_film[str(l)](h_dev)   # (B, 2*D_dev)\n",
    "            site_params = self.site_film[str(l)](h_site) # (B, 2*D_site)\n",
    "            rest_params = self.rest_film[str(l)](h_rest) # (B, 2*D_rest)\n",
    "\n",
    "            g_dev, b_dev = dev_params.chunk(2, dim=-1)\n",
    "            g_site, b_site = site_params.chunk(2, dim=-1)\n",
    "            g_rest, b_rest = rest_params.chunk(2, dim=-1)\n",
    "\n",
    "            gamma_dev[l], beta_dev[l] = g_dev, b_dev\n",
    "            gamma_site[l], beta_site[l] = g_site, b_site\n",
    "            gamma_rest[l], beta_rest[l] = g_rest, b_rest\n",
    "\n",
    "        # Unroll ViT blocks with FiLM++ after MHSA\n",
    "        for layer_idx, blk in enumerate(self.v.blocks):\n",
    "            attn_out = blk.attn(blk.norm1(x))\n",
    "            x = x + blk.drop_path(attn_out)   # (B, T, D_total)\n",
    "\n",
    "            if layer_idx in self.conditioned_layers_set:\n",
    "                gammas = {\n",
    "                    \"dev\": gamma_dev[layer_idx],\n",
    "                    \"site\": gamma_site[layer_idx],\n",
    "                    \"rest\": gamma_rest[layer_idx],\n",
    "                }\n",
    "                betas = {\n",
    "                    \"dev\": beta_dev[layer_idx],\n",
    "                    \"site\": beta_site[layer_idx],\n",
    "                    \"rest\": beta_rest[layer_idx],\n",
    "                }\n",
    "                x = self._apply_filmpp_grouped(x, gammas, betas)\n",
    "\n",
    "            x = x + blk.drop_path(blk.mlp(blk.norm2(x)))\n",
    "\n",
    "        x = self.v.norm(x)\n",
    "        h_cls = (x[:, 0] + x[:, 1]) / 2\n",
    "        return h_cls\n",
    "\n",
    "    def forward(self, x, m_dev, m_site, m_rest):\n",
    "        \"\"\"\n",
    "        x:      (B, T, F) or (B, 1, F, T)\n",
    "        m_dev:  (B, dev_metadata_dim)\n",
    "        m_site: (B, site_metadata_dim)\n",
    "        m_rest: (B, rest_metadata_dim)\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1).transpose(2, 3)\n",
    "\n",
    "        h_cls = self.forward_features(x, m_dev, m_site, m_rest)\n",
    "        logits = self.classifier(h_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "astfilmpp = ASTFiLMPlusPlus(\n",
    "    dev_metadata_dim=4,\n",
    "    site_metadata_dim=7,\n",
    "    rest_metadata_dim=3,\n",
    "    D_dev=128,\n",
    "    D_site=128,\n",
    "    ast_kwargs=ast_kwargs,\n",
    "    conditioned_layers=(10, 11, 12), # last 3 layers\n",
    "    metadata_hidden_dim=64, \n",
    "    film_hidden_dim=64,\n",
    "    dropout_p=0.3, \n",
    "    num_labels=2\n",
    ").to(DEVICE)\n",
    "print(astfilmpp)\n",
    "\n",
    "dev_metadata = torch.randn(2, 4).to(DEVICE)\n",
    "site_metadata = torch.randn(2, 7).to(DEVICE)\n",
    "rest_metadata = torch.randn(2, 3).to(DEVICE)\n",
    "out = astfilmpp(dummy_input, dev_metadata, site_metadata, rest_metadata)  # (2, 2)\n",
    "print(out.shape)  # torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740224d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icbhi-ast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
