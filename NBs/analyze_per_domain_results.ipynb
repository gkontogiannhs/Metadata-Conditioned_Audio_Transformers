{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_sensitivity_specificity_scatter(csv_path, metric=\"icbhi_score\", figsize=(8,6), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a sensitivity vs specificity scatter per device and per site.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to your 'group_performance_summary_foldX.csv'\n",
    "    metric : str\n",
    "        Metric to use for color (e.g., 'icbhi_score' or 'f1_macro')\n",
    "    figsize : tuple\n",
    "        Size of the matplotlib figure\n",
    "    save_path : str or None\n",
    "        Optional path to save the figure (PNG)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "    df = df.reset_index().rename(columns={\"index\": \"group\"})\n",
    "\n",
    "    # Split type (device/site)\n",
    "    df[\"type\"] = df[\"group\"].apply(lambda x: \"Device\" if x.startswith(\"device\") else \"Site\")\n",
    "    df[\"name\"] = df[\"group\"].apply(lambda x: x.split(\"::\")[1])\n",
    "\n",
    "    # Sanity filter\n",
    "    df = df.dropna(subset=[\"sensitivity\", \"specificity\", metric])\n",
    "\n",
    "    # Plot setup\n",
    "    sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Scatter points\n",
    "    sns.scatterplot(\n",
    "        data=df, x=\"specificity\", y=\"sensitivity\",\n",
    "        hue=metric, style=\"type\", s=200, edgecolor=\"black\", palette=\"coolwarm\", ax=ax\n",
    "    )\n",
    "\n",
    "    # Annotate with device/site names\n",
    "    for _, row in df.iterrows():\n",
    "        ax.text(row[\"specificity\"] + 0.002, row[\"sensitivity\"] + 0.002,\n",
    "                row[\"name\"], fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Reference lines (mean Sp, Se)\n",
    "    ax.axvline(df[\"specificity\"].mean(), color=\"gray\", ls=\"--\", lw=1)\n",
    "    ax.axhline(df[\"sensitivity\"].mean(), color=\"gray\", ls=\"--\", lw=1)\n",
    "\n",
    "    ax.set_xlim(0.6, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlabel(\"Specificity (True Negative Rate)\")\n",
    "    ax.set_ylabel(\"Sensitivity (True Positive Rate)\")\n",
    "    ax.set_title(\"Domain Bias Analysis: Sensitivity vs Specificity per Device/Site\")\n",
    "\n",
    "    # Legend & layout\n",
    "    plt.legend(title=metric, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Saved figure to: {save_path}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98331a8",
   "metadata": {},
   "source": [
    "Each point = one device or site:\n",
    "\n",
    "X-axis: Specificity → right = fewer false positives.\n",
    "\n",
    "Y-axis: Sensitivity → up = better detection of abnormalities.\n",
    "\n",
    "Color: ICBHI (HS) or F1-macro score.\n",
    "\n",
    "Marker style: circles for devices, triangles for sites.\n",
    "\n",
    "Dashed lines: average Sp and Se (visualize bias region).\n",
    "\n",
    "How to read it\n",
    "\n",
    "Top-right quadrant: balanced & robust domain (ideal).\n",
    "\n",
    "Bottom-right: conservative (few false positives, misses many positives).\n",
    "\n",
    "Top-left: noisy (detects everything, but poor precision).\n",
    "\n",
    "Bottom-left: failure zone (poor both ways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity_specificity_scatter(\n",
    "    \"summaries/group_performance_summary_fold0.csv\",\n",
    "    metric=\"icbhi_score\",\n",
    "    save_path=\"figures/sp_vs_se_per_group.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fc0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Recall(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_true[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_pred[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def F1Measure(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
    "            continue\n",
    "        temp+= (2*sum(np.logical_and(y_true[i], y_pred[i])))/ (sum(y_true[i])+sum(y_pred[i]))\n",
    "    return temp/ y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3a7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.array([[0,1],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [0,0]])\n",
    "\n",
    "all_preds = np.array([[0,1],\n",
    "                   [0,1],\n",
    "                   [0,1],\n",
    "                   [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aceaafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(Precision(all_labels, all_preds))\n",
    "print(Recall(all_labels, all_preds))\n",
    "print(F1Measure(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40adc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_multilabel_metrics(all_labels, all_preds, all_probs=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Compute detailed multi-label metrics + official ICBHI metrics +\n",
    "    macro-averaged metrics + binary Normal-vs-Abnormal ICBHI score.\n",
    "\n",
    "    Works for multilabel setup with two binary labels [crackle, wheeze],\n",
    "    forming 4 composite classes (Normal, Crackle, Wheeze, Both).\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # === Composite 4-class masks (true/pred) ===\n",
    "    is_n = (all_labels[:,0]==0) & (all_labels[:,1]==0)\n",
    "    is_c = (all_labels[:,0]==1) & (all_labels[:,1]==0)\n",
    "    is_w = (all_labels[:,0]==0) & (all_labels[:,1]==1)\n",
    "    is_b = (all_labels[:,0]==1) & (all_labels[:,1]==1)\n",
    "\n",
    "    pr_n = (all_preds[:,0]==0) & (all_preds[:,1]==0)\n",
    "    pr_c = (all_preds[:,0]==1) & (all_preds[:,1]==0)\n",
    "    pr_w = (all_preds[:,0]==0) & (all_preds[:,1]==1)\n",
    "    pr_b = (all_preds[:,0]==1) & (all_preds[:,1]==1)\n",
    "\n",
    "    # === Class-wise totals ===\n",
    "    Nn, Nc, Nw, Nb = is_n.sum(), is_c.sum(), is_w.sum(), is_b.sum()\n",
    "    Pn = np.sum(is_n & pr_n)\n",
    "    Pc = np.sum(is_c & pr_c)\n",
    "    Pw = np.sum(is_w & pr_w)\n",
    "    Pb = np.sum(is_b & pr_b)\n",
    "\n",
    "    # === Official 4-class ICBHI metrics ===\n",
    "    sp = Pn / (Nn + 1e-12)\n",
    "    se = (Pc + Pw + Pb) / (Nc + Nw + Nb + 1e-12)\n",
    "    hs = 0.5 * (sp + se)\n",
    "    metrics.update({'icbhi_specificity': sp, 'icbhi_sensitivity': se, 'icbhi_score': hs})\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[ICBHI 4-class] SPE={sp*100:.2f}% | SEN={se*100:.2f}% | HS={hs*100:.2f}%\")\n",
    "\n",
    "    # === Per-class metrics ===\n",
    "    pattern_names = ['Normal', 'Crackle', 'Wheeze', 'Both']\n",
    "    is_true = [is_n, is_c, is_w, is_b]\n",
    "    is_pred = [pr_n, pr_c, pr_w, pr_b]\n",
    "    Ps = [Pn, Pc, Pw, Pb]\n",
    "    Ns = [Nn, Nc, Nw, Nb]\n",
    "\n",
    "    total_samples = len(all_labels)\n",
    "    per_class = []\n",
    "\n",
    "    for name, tmask, pmask, P, N in zip(pattern_names, is_true, is_pred, Ps, Ns):\n",
    "        TP = P\n",
    "        FN = N - P\n",
    "        FP = np.sum(~tmask & pmask)\n",
    "        TN = total_samples - TP - FP - FN\n",
    "\n",
    "        precision = TP / (TP + FP + 1e-12)\n",
    "        recall = TP / (TP + FN + 1e-12)         # sensitivity\n",
    "        specificity = TN / (TN + FP + 1e-12)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-12)\n",
    "\n",
    "        per_class.append({\n",
    "            'name': name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'specificity': specificity,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'support': N\n",
    "        })\n",
    "\n",
    "        metrics.update({\n",
    "            f'{name}_precision': precision,\n",
    "            f'{name}_recall': recall,\n",
    "            f'{name}_specificity': specificity,\n",
    "            f'{name}_f1': f1,\n",
    "            f'{name}_accuracy': accuracy\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{name}] P={precision*100:.2f}% | R={recall*100:.2f}% | \"\n",
    "                  f\"Sp={specificity*100:.2f}% | F1={f1*100:.2f}% | Acc={accuracy*100:.2f}%\")\n",
    "\n",
    "    # === Macro and Weighted averages ===\n",
    "    precisions = [c['precision'] for c in per_class]\n",
    "    recalls = [c['recall'] for c in per_class]\n",
    "    specificities = [c['specificity'] for c in per_class]\n",
    "    f1s = [c['f1'] for c in per_class]\n",
    "    accuracies = [c['accuracy'] for c in per_class]\n",
    "    supports = [c['support'] for c in per_class]\n",
    "\n",
    "    metrics.update({\n",
    "        'macro_precision': np.mean(precisions),\n",
    "        'macro_recall': np.mean(recalls),\n",
    "        'macro_specificity': np.mean(specificities),\n",
    "        'macro_f1': np.mean(f1s),\n",
    "        'macro_accuracy': np.mean(accuracies),\n",
    "        'weighted_precision': np.average(precisions, weights=supports),\n",
    "        'weighted_recall': np.average(recalls, weights=supports),\n",
    "        'weighted_specificity': np.average(specificities, weights=supports),\n",
    "        'weighted_f1': np.average(f1s, weights=supports),\n",
    "        'weighted_accuracy': np.average(accuracies, weights=supports)\n",
    "    })\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[Macro averages]\")\n",
    "        print(f\"P={metrics['macro_precision']*100:.2f}% | R={metrics['macro_recall']*100:.2f}% | \"\n",
    "              f\"Sp={metrics['macro_specificity']*100:.2f}% | F1={metrics['macro_f1']*100:.2f}% | \"\n",
    "              f\"Acc={metrics['macro_accuracy']*100:.2f}%\")\n",
    "\n",
    "    # === Binary Normal vs Abnormal (ICBHI style) ===\n",
    "    is_abn_true = ~is_n\n",
    "    is_abn_pred = ~pr_n\n",
    "\n",
    "    TP = np.sum(is_abn_true & is_abn_pred)  # correctly abnormal\n",
    "    TN = np.sum(is_n & pr_n)                # correctly normal\n",
    "    FP = np.sum(~is_abn_true & is_abn_pred) # predicted abnormal but actually normal\n",
    "    FN = np.sum(is_abn_true & ~is_abn_pred) # predicted normal but actually abnormal\n",
    "\n",
    "    binary_spe = TN / (TN + FP + 1e-12)\n",
    "    binary_sen = TP / (TP + FN + 1e-12)\n",
    "    binary_hs = 0.5 * (binary_spe + binary_sen)\n",
    "\n",
    "    metrics.update({\n",
    "        'binary_specificity': binary_spe,\n",
    "        'binary_sensitivity': binary_sen,\n",
    "        'binary_icbhi_score': binary_hs\n",
    "    })\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[Binary Normal-vs-Abnormal ICBHI]\")\n",
    "        print(f\"SPE={binary_spe*100:.2f}% | SEN={binary_sen*100:.2f}% | HS={binary_hs*100:.2f}%\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c481ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ICBHI official] SPE=50.00% | SEN=50.00% | HS=50.00%\n",
      "[Normal] P=100.00% | R=50.00% | Sp=100.00% | F1=66.67%\n",
      "[Crackle] P=33.33% | R=50.00% | Sp=50.00% | F1=40.00%\n",
      "[Wheeze] P=100.00% | R=100.00% | Sp=100.00% | F1=100.00%\n",
      "[Both] P=0.00% | R=0.00% | Sp=80.00% | F1=0.00%\n",
      "\n",
      "Summary: {'Normal_f1': np.float64(0.667), 'Crackle_f1': np.float64(0.4), 'Wheeze_f1': np.float64(1.0), 'Both_f1': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "all_labels = np.array([\n",
    "    [0,0],  # Normal\n",
    "    [1,0],  # Crackle\n",
    "    [0,1],  # Wheeze\n",
    "    [1,1],  # Both\n",
    "    [0,0],  # Normal\n",
    "    [1,0],  # Crackle\n",
    "])\n",
    "all_preds = np.array([\n",
    "    [0,0],  # Normal\n",
    "    [1,0],  # Crackle\n",
    "    [0,1],  # Wheeze\n",
    "    [1,0],  # Crackle\n",
    "    [1,0],  # Crackle\n",
    "    [1,1],  # Both\n",
    "])\n",
    "\n",
    "\n",
    "m = compute_multilabel_metrics(all_labels, all_preds, verbose=True)\n",
    "print(\"\\nSummary:\", {k: round(v, 3) for k, v in m.items() if \"f1\" in k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85208e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multilabel_metrics2(all_labels, all_preds, all_probs, verbose=True):\n",
    "    \"\"\"\n",
    "    Compute detailed multi-label metrics + official ICBHI metrics.\n",
    "    Now includes Sensitivity/Specificity for Normal and Both composite classes.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # === Composite 4-class masks ===\n",
    "    is_n = (all_labels[:,0]==0) & (all_labels[:,1]==0)\n",
    "    is_c = (all_labels[:,0]==1) & (all_labels[:,1]==0)\n",
    "    is_w = (all_labels[:,0]==0) & (all_labels[:,1]==1)\n",
    "    is_b = (all_labels[:,0]==1) & (all_labels[:,1]==1)\n",
    "\n",
    "    pr_n = (all_preds[:,0]==0) & (all_preds[:,1]==0)\n",
    "    pr_c = (all_preds[:,0]==1) & (all_preds[:,1]==0)\n",
    "    pr_w = (all_preds[:,0]==0) & (all_preds[:,1]==1)\n",
    "    pr_b = (all_preds[:,0]==1) & (all_preds[:,1]==1)\n",
    "\n",
    "    # === Counts ===\n",
    "    Nn, Nc, Nw, Nb = is_n.sum(), is_c.sum(), is_w.sum(), is_b.sum()\n",
    "    Pn = np.sum(is_n & pr_n)\n",
    "    Pc = np.sum(is_c & pr_c)\n",
    "    Pw = np.sum(is_w & pr_w)\n",
    "    Pb = np.sum(is_b & pr_b)\n",
    "\n",
    "    # === Per-pattern metrics (Normal, Crackle, Wheeze, Both) ===\n",
    "    pattern_names = ['Normal', 'Crackle', 'Wheeze', 'Both']\n",
    "    is_true = [is_n, is_c, is_w, is_b]\n",
    "    is_pred = [pr_n, pr_c, pr_w, pr_b]\n",
    "\n",
    "    for name, tmask, pmask in zip(pattern_names, is_true, is_pred):\n",
    "        tp = np.sum(tmask & pmask)\n",
    "        fp = np.sum(~tmask & pmask)\n",
    "        fn = np.sum(tmask & ~pmask)\n",
    "        tn = np.sum(~tmask & ~pmask)\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-12)\n",
    "        recall = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "        # acc = (tp + tn) / (tp + tn + fp + fn + 1e-12)\n",
    "        # sensitivity_c = recall  # TP / (TP+FN)\n",
    "        specificity_c = tn / (tn + fp + 1e-12)\n",
    "\n",
    "        print(f\"[{name}] P={precision*100:.2f}% | R={recall*100:.2f}% | \"\n",
    "              f\"Sp={specificity_c*100:.2f}% | F1={f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64cf4b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normal] P=100.00% | R=50.00% | Sp=100.00% | F1=66.67%\n",
      "[Crackle] P=33.33% | R=50.00% | Sp=50.00% | F1=40.00%\n",
      "[Wheeze] P=100.00% | R=100.00% | Sp=100.00% | F1=100.00%\n",
      "[Both] P=0.00% | R=0.00% | Sp=80.00% | F1=0.00%\n"
     ]
    }
   ],
   "source": [
    "compute_multilabel_metrics2(all_labels, all_preds, None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d446e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icbhi-ast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
