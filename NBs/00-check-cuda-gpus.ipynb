{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2325e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ls.engine.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f69e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 8\n",
      "[Device] Using CUDA:7 -> NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_device(device_id=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b090ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ls.config.loader import load_config\n",
    "\n",
    "cfg = load_config(\"../configs/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eda98ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box({'device_id': 0, 'use_dataparallel': True, 'gpus': '2, 3, 4'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.training.hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89158f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def set_visible_gpus(gpus: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Restrict which GPUs PyTorch can see by setting CUDA_VISIBLE_DEVICES.\n",
    "\n",
    "    Args:\n",
    "        gpus (str): Comma-separated GPU indices, e.g., \"0,1,2,3\".\n",
    "        verbose (bool): If True, print the selection info.\n",
    "    \"\"\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "    if verbose:\n",
    "        print(f\"[CUDA] Visible devices set to: {gpus}\")\n",
    "\n",
    "    # Optional sanity check after setting\n",
    "    torch.cuda.device_count()  # forces CUDA to reinitialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5156ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(device_id: int = 0, verbose: bool = True) -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the best available device among CUDA, MPS, and CPU.\n",
    "    Automatically detects hardware availability.\n",
    "\n",
    "    Args:\n",
    "        device_id (int): Index of visible CUDA device to use.\n",
    "        verbose (bool): If True, print chosen device.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: torch.device(\"cuda\"|\"mps\"|\"cpu\")\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        if device_id >= num_devices:\n",
    "            raise ValueError(f\"Requested CUDA device {device_id}, but only {num_devices} available.\")\n",
    "        device = torch.device(f\"cuda:{device_id}\")\n",
    "        if verbose:\n",
    "            print(f\"[Device] Using CUDA:{device_id} → {torch.cuda.get_device_name(device_id)}\")\n",
    "\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "        if verbose:\n",
    "            print(\"[Device] Using Apple Metal (MPS) acceleration\")\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if verbose:\n",
    "            print(\"[Device] Using CPU (no GPU backend found)\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc2300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] Using CUDA:0 → NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.training.hardware.device = get_device(device_id=cfg.training.hardware.device_id)\n",
    "cfg.training.hardware.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf93b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_device(model: nn.Module, device: torch.device, use_dataparallel: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Moves the model to the selected device.\n",
    "    Optionally wraps it with DataParallel for multi-GPU use.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to place on device.\n",
    "        device (torch.device): Device from get_device().\n",
    "        use_dataparallel (bool): If True and multiple GPUs visible, wrap model in DataParallel.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Model ready for training on chosen device(s).\n",
    "    \"\"\"\n",
    "    if use_dataparallel and torch.cuda.device_count() > 1:\n",
    "        print(f\"[Model] Using {torch.cuda.device_count()} GPUs via DataParallel\")\n",
    "        model = nn.DataParallel(model)\n",
    "    else:\n",
    "        print(f\"[Model] Using single device: {device}\")\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdcc20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icbhi-ast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
