seed: 42

dataset:
  name: icbhi
  data_folder: /home/AIoT04/Datasets/icbhi_dataset
  cycle_metadata_path: /home/AIoT04/Datasets/icbhi_dataset/icbhi_metadata.csv
  class_split: lungsound
  split_strategy: official # official, foldwise, random (patient-wise random)
  test_fold: 0
  multi_label: true
  n_cls: 4
  weighted_sampler: false
  batch_size: 16
  num_workers: 0
  h: 128
  w: 1024

audio:
  # audio basics
  sample_rate: 16000
  desired_length: 10.0
  remove_dc: true
  normalize: false
  # padding type: zero, repeat
  pad_type: repeat
  # fade settings
  use_fade: true
  fade_samples_ratio: 64
  # features
  n_mels: 128
  frame_length: 40
  frame_shift: 10
  low_freq: 100
  high_freq: 8000
  window_type: hanning
  use_energy: false
  dither: 0.0
  mel_norm: mit # hf, mit, None
  
  resz: 1.0 # spectrogram resize factor (1.0 = no resize)
  # Augmentation
  raw_augment: 1
  wave_aug:
    - type: Crop
      sampling_rate: 16000
      zone: [0.0, 1.0]
      coverage: 1.0
      p: 0.0

    - type: Noise
      color: white
      p: 0.1

    - type: Speed
      factor: [0.9, 1.1]
      p: 0.1

    - type: Loudness
      factor: [0.5, 2.0]
      p: .1

    - type: VTLP
      sampling_rate: 16000
      zone: [0.0, 1.0]
      fhi: 4800
      factor: [0.9, 1.1]
      p: 0.1

    - type: Pitch
      sampling_rate: 16000
      factor: [-1, 3]
      p: 0.0

  spec_aug:
    - type: SpecAugment
      policy: icbhi_ast_sup
      mask: zero
      p: .3

models:
  ast_film_soft:
    name: ast_film_soft
    # AST settings
    audioset_pretrain: true
    input_fdim: 128
    input_tdim: 1024
    model_size: base384
    fstride: 10
    tstride: 10
    imagenet_pretrain: true
    label_dim: 2
    audioset_ckpt_path: /home/AIoT04/Dev/pretrained_models/audioset_10_10_0.4593.pth
    dropout: 0.4
    
    # FiLM++ settings
    dev_emb_dim: 8
    site_emb_dim: 14
    metadata_hidden_dim: 64
    film_hidden_dim: 64
    
    # Soft mask settings
    mask_init_scale: 0.5
    mask_sparsity_lambda: 0.01    # Regularization for mask overlap
    mask_coverage_lambda: 0.005   # Regularization for dimension coverage
    per_layer_masks: false
    mask_temperature: 1.0         # Initial temperature (annealed during training)
    mask_temperature_end: 0.2     # Final temperature after annealing
    mask_temperature_warmup: 5    # Epochs to keep temperature high
    mask_lr_multiplier: 10.0      # LR multiplier for mask parameters
    
    # Generator init
    film_init_gain: 0.1
    
    # Conditioned layers
    conditioned_layers: [10, 11]
    
    debug_film: false

training:
  # model_name: ast
  n_cls: 4
  loss: bce # bce, weighted_ce, focal, label_smoothing, asymmetric, hierarchical, composite
  sensitivity_bias: 1.5
  mask_sparsity_lambda: 0.01
  alpha: 0.25
  gamma: 2.0
  use_class_weights: false
  epochs: 50
  kfold_splits: 5
  early_stopping: 5
  grad_clip: 1.0

  dev_emb_dim: 8
  site_emb_dim: 14
  metadata_hidden_dim: 64
  film_hidden_dim: 64
  
  # Soft mask settings
  mask_init_scale: 0.5          # v2: moderate init (was 2.0 — caused saturation)
  mask_sparsity_lambda: 0.01    # Regularization for mask overlap
  mask_coverage_lambda: 0.005   # Regularization for dimension coverage
  per_layer_masks: false
  mask_temperature: 1.0         # Initial temperature (annealed during training)
  mask_temperature_end: 0.2     # Final temperature after annealing
  mask_temperature_warmup: 5    # Epochs to keep temperature high
  mask_lr_multiplier: 10.0      # LR multiplier for mask parameters
  
  # Generator init
  film_init_gain: 0.1           # v2: small nonzero init (was 0.0 — caused zero gradients)
  
  # Conditioned layers
  conditioned_layers: [10, 11]
  
  debug_film: false

  # AST Freezing
  freeze:
    strategy: none  # none, all, until_block, trainable_blocks
    trainable_blocks:         # Number of blocks to keep trainable (from end)
    until_block:          # Alternative: freeze blocks 0-9


  # Loss-specific parameters (for hierarchical/composite)
  partial_credit: 0.5
  miss_penalty: 2.0
  over_pred_cost: 0.3

  hardware:
    visible_gpus: "0, 1, 2, 3, 4, 5, 6, 7, 8, 9"   # which GPUs to expose
    device_id: 0          # which GPU among visible ones to use
    use_dataparallel: false    # enable/disable DataParallel

  optimizer:
    type: adamw
    lr: 3e-5
    weight_decay: 0.05 # only used if scheduler.cosine_weight_decay=false
    # momentum:
    # betas: # [0.9, 0.95]
    cosine_weight_decay: true
    final_weight_decay: 0.0

    # Differential LR
    use_differential_lr: false
    backbone_lr_scale: 0.01
    embedding_lr_scale: 0.5
    encoder_lr_scale: 1.0
    film_lr_scale: 1.0
    mask_lr_scale: 1.5       # Slightly higher for masks
    classifier_lr_scale: 2.0

  scheduler:
    type: cosine_warmup        # options: none, cosine, cosine_warmup, reduce_on_plateau, cosine_warmup_restarts
    warmup_epochs: 10         # used only for warmup
    T_0: 20               # used only for cosine_*
    start_linear_warmup: 1e-4
    end_linear_warmup: 1
    min_lr: 1e-8

    reduce_factor: 0.5        # used for ReduceLROnPlateau
    reduce_patience: 2
    reduce_min_lr: 1e-8
    reduce_metric: "val_loss"  # metric to monitor
    reduce_mode: "min"        # or "min", depending on metric (e.g., loss=min)

mlflow:
  tracking_uri:
  tracking_username:
  tracking_password:
  experiment_name:
